#!/usr/bin/env python3
"""
Context Manager - ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

Codex Agent Loop ìµœì í™” ê¸°ë²• + ì¸ê³¼ê´€ê³„ ê·¸ë˜í”„ë¥¼ ê²°í•©í•˜ì—¬
context compaction ì‹œì—ë„ ì¤‘ìš” ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- Causality Graph: ë…¸ë“œ ê°„ ì¸ê³¼ê´€ê³„ ì¶”ì  (causes, leads_to, resolved_by)
- Importance Scoring: HIGH/MEDIUM/LOW ê¸°ë°˜ compaction ìš°ì„ ìˆœìœ„
- ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©: phase_gate, auto_state, noteì™€ ì—°ë™
- Compact Summary: compaction ìƒì¡´ìš© ì••ì¶• ìš”ì•½ ìë™ ìƒì„±

ë…¸ë“œ ìœ í˜•:
- REQUEST: ì‚¬ìš©ì ìš”ì²­
- ANALYSIS: ë¶„ì„ ê²°ê³¼
- DECISION: ê²°ì • ì‚¬í•­
- REJECTED: ê±°ë¶€ëœ ëŒ€ì•ˆ
- FILE: íŒŒì¼ ë³€ê²½
- ERROR: ë°œìƒí•œ ì—ëŸ¬
- SOLUTION: í•´ê²°ì±…
- LEARNING: í•™ìŠµ ë‚´ìš©/íŒ¨í„´

ì—£ì§€ ìœ í˜•:
- causes: Aê°€ Bë¥¼ ìœ ë°œ
- leads_to: Aê°€ Bë¡œ ì´ì–´ì§
- resolved_by: A(ì—ëŸ¬)ê°€ B(ì†”ë£¨ì…˜)ë¡œ í•´ê²°
- blocks: Aê°€ Bë¥¼ ì°¨ë‹¨
- depends_on: Aê°€ Bì— ì˜ì¡´
- rejects: Aê°€ Bë¥¼ ê±°ë¶€ (ëŒ€ì•ˆ ê¸°ë¡)
"""

import json
import os
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Optional


# ë™ì  í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê°ì§€
def _get_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€"""
    if env_root := os.environ.get("CLAUDE_PROJECT_DIR"):
        return Path(env_root)
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent.parent.parent.parent
    if (project_root / ".claude").exists():
        return project_root
    return Path.cwd()


PROJECT_ROOT = _get_project_root()
CONTEXT_DIR = PROJECT_ROOT / ".omc" / "context"
CONTEXT_FILE = CONTEXT_DIR / "context_graph.json"
SUMMARY_FILE = CONTEXT_DIR / "compact_summary.md"
ARCHIVE_DIR = CONTEXT_DIR / "archive"

# === íŒŒì¼ í¬ê¸° ì œí•œ ì„¤ì • ===
MAX_NODES = 500                    # ìµœëŒ€ ë…¸ë“œ ìˆ˜
MAX_EDGES = 1000                   # ìµœëŒ€ ì—£ì§€ ìˆ˜
MAX_FILE_SIZE_KB = 1024            # ìµœëŒ€ íŒŒì¼ í¬ê¸° (1MB)
TTL_HOURS = 24                     # ë…¸ë“œ TTL (24ì‹œê°„)
CLEANUP_THRESHOLD_PERCENT = 80     # ì´ ë¹„ìœ¨ ì´ˆê³¼ ì‹œ ì •ë¦¬ ì‹œì‘
ARCHIVE_BATCH_SIZE = 100           # ì•„ì¹´ì´ë¸Œ ì‹œ ë°°ì¹˜ í¬ê¸°

# === v13.0 ì„¤ì • ===
WISDOM_DIR = PROJECT_ROOT / ".omc" / "notepads"
CIRCUIT_BREAKER_FILE = PROJECT_ROOT / ".omc" / "state" / "circuit-breaker.json"
VERIFICATION_FRESHNESS_SECONDS = 300  # 5ë¶„


class NodeType(Enum):
    """ì»¨í…ìŠ¤íŠ¸ ë…¸ë“œ ìœ í˜•"""
    REQUEST = "request"          # ì‚¬ìš©ì ìš”ì²­
    ANALYSIS = "analysis"        # ë¶„ì„ ê²°ê³¼
    DECISION = "decision"        # ê²°ì • ì‚¬í•­
    REJECTED = "rejected"        # ê±°ë¶€ëœ ëŒ€ì•ˆ
    FILE = "file"                # íŒŒì¼ ë³€ê²½
    ERROR = "error"              # ë°œìƒí•œ ì—ëŸ¬
    SOLUTION = "solution"        # í•´ê²°ì±…
    LEARNING = "learning"        # í•™ìŠµ ë‚´ìš©


class EdgeType(Enum):
    """ì»¨í…ìŠ¤íŠ¸ ì—£ì§€ ìœ í˜•"""
    CAUSES = "causes"            # Aê°€ Bë¥¼ ìœ ë°œ
    LEADS_TO = "leads_to"        # Aê°€ Bë¡œ ì´ì–´ì§
    RESOLVED_BY = "resolved_by"  # A(ì—ëŸ¬)ê°€ B(ì†”ë£¨ì…˜)ë¡œ í•´ê²°
    BLOCKS = "blocks"            # Aê°€ Bë¥¼ ì°¨ë‹¨
    DEPENDS_ON = "depends_on"    # Aê°€ Bì— ì˜ì¡´
    REJECTS = "rejects"          # Aê°€ Bë¥¼ ê±°ë¶€


class Importance(Enum):
    """ì¤‘ìš”ë„ ë ˆë²¨ (compaction ìš°ì„ ìˆœìœ„)"""
    HIGH = "high"      # ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€
    MEDIUM = "medium"  # ê°€ëŠ¥í•˜ë©´ ë³´ì¡´
    LOW = "low"        # í•„ìš” ì‹œ ì‚­ì œ ê°€ëŠ¥


# === v13.0: CircuitBreaker ===

class CircuitBreaker:
    """
    3-Failure ì—ëŸ¬ ì—ìŠ¤ì»¬ë ˆì´ì…˜ íŒ¨í„´

    ê°™ì€ íƒœìŠ¤í¬ì—ì„œ ì—°ì† 3ë²ˆ ì‹¤íŒ¨í•˜ë©´ Architectì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

    ì‹¤íŒ¨ ì²˜ë¦¬ íë¦„:
    1. 1ì°¨ ì‹¤íŒ¨: RETRY (ì¬ì‹œë„)
    2. 2ì°¨ ì‹¤íŒ¨: ALTERNATE_APPROACH (ë‹¤ë¥¸ ì ‘ê·¼ë²• ì‹œë„)
    3. 3ì°¨ ì‹¤íŒ¨: ESCALATE_TO_ARCHITECT (Architectì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜)
    """

    def __init__(self, max_failures: int = 3):
        """
        Args:
            max_failures: ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì „ ìµœëŒ€ ì‹¤íŒ¨ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        """
        self.max_failures = max_failures
        self.failure_counts: dict[str, int] = {}  # {task_key: count}
        self.failure_history: dict[str, list] = {}  # {task_key: [errors]}

    def record_failure(self, task_key: str, error: str) -> str:
        """
        ì‹¤íŒ¨ ê¸°ë¡ ë° ë‹¤ìŒ ì•¡ì…˜ ê²°ì •

        Args:
            task_key: íƒœìŠ¤í¬ ì‹ë³„ì
            error: ì—ëŸ¬ ë©”ì‹œì§€

        Returns:
            "RETRY" | "ALTERNATE_APPROACH" | "ESCALATE_TO_ARCHITECT"
        """
        # ì¹´ìš´í„° ì¦ê°€
        self.failure_counts[task_key] = self.failure_counts.get(task_key, 0) + 1
        count = self.failure_counts[task_key]

        # ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ê¸°ë¡
        if task_key not in self.failure_history:
            self.failure_history[task_key] = []
        self.failure_history[task_key].append({
            "error": error,
            "timestamp": datetime.now().isoformat(),
            "attempt": count,
        })

        # ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
        if count == 1:
            return "RETRY"
        elif count == 2:
            return "ALTERNATE_APPROACH"
        else:  # count >= 3
            return "ESCALATE_TO_ARCHITECT"

    def reset(self, task_key: str):
        """
        íƒœìŠ¤í¬ ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹

        Args:
            task_key: íƒœìŠ¤í¬ ì‹ë³„ì
        """
        if task_key in self.failure_counts:
            del self.failure_counts[task_key]
        if task_key in self.failure_history:
            del self.failure_history[task_key]

    def get_failure_summary(self, task_key: str) -> dict:
        """
        ì‹¤íŒ¨ ì´ë ¥ ìš”ì•½

        Args:
            task_key: íƒœìŠ¤í¬ ì‹ë³„ì

        Returns:
            {
                "task_key": str,
                "failure_count": int,
                "errors": list[dict],
                "recommendation": str
            }
        """
        count = self.failure_counts.get(task_key, 0)
        history = self.failure_history.get(task_key, [])

        if count == 0:
            recommendation = "NO_FAILURES"
        elif count == 1:
            recommendation = "RETRY"
        elif count == 2:
            recommendation = "ALTERNATE_APPROACH"
        else:
            recommendation = "ESCALATE_TO_ARCHITECT"

        return {
            "task_key": task_key,
            "failure_count": count,
            "errors": history,
            "recommendation": recommendation,
        }

    def to_dict(self) -> dict:
        """ì§ë ¬í™”"""
        return {
            "max_failures": self.max_failures,
            "failure_counts": self.failure_counts,
            "failure_history": self.failure_history,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "CircuitBreaker":
        """ì—­ì§ë ¬í™”"""
        cb = cls(max_failures=data.get("max_failures", 3))
        cb.failure_counts = data.get("failure_counts", {})
        cb.failure_history = data.get("failure_history", {})
        return cb

    def get_all_failures(self) -> dict[str, dict]:
        """
        ëª¨ë“  íƒœìŠ¤í¬ì˜ ì‹¤íŒ¨ ìƒíƒœ ì¡°íšŒ

        Returns:
            {task_key: failure_summary}
        """
        result = {}
        for task_key in self.failure_counts:
            result[task_key] = self.get_failure_summary(task_key)
        return result


# === v13.0: VerificationResult ===

@dataclass
class VerificationResult:
    """ê²€ì¦ ê²°ê³¼"""
    check_type: str  # "BUILD", "TEST", "LINT", "FUNCTIONALITY", "ARCHITECT", "TODO", "ERROR_FREE"
    passed: bool
    evidence: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> dict:
        return {
            "check_type": self.check_type,
            "passed": self.passed,
            "evidence": self.evidence,
            "timestamp": self.timestamp,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "VerificationResult":
        return cls(
            check_type=data["check_type"],
            passed=data["passed"],
            evidence=data["evidence"],
            timestamp=data.get("timestamp", datetime.now().isoformat()),
        )


@dataclass
class ContextNode:
    """ì»¨í…ìŠ¤íŠ¸ ê·¸ë˜í”„ ë…¸ë“œ"""
    id: str
    type: NodeType
    content: str
    importance: Importance = Importance.MEDIUM
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: dict = field(default_factory=dict)

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "type": self.type.value,
            "content": self.content,
            "importance": self.importance.value,
            "timestamp": self.timestamp,
            "metadata": self.metadata,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "ContextNode":
        return cls(
            id=data["id"],
            type=NodeType(data["type"]),
            content=data["content"],
            importance=Importance(data.get("importance", "medium")),
            timestamp=data.get("timestamp", datetime.now().isoformat()),
            metadata=data.get("metadata", {}),
        )


@dataclass
class ContextEdge:
    """ì»¨í…ìŠ¤íŠ¸ ê·¸ë˜í”„ ì—£ì§€"""
    source_id: str
    target_id: str
    type: EdgeType
    description: str = ""
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> dict:
        return {
            "source_id": self.source_id,
            "target_id": self.target_id,
            "type": self.type.value,
            "description": self.description,
            "timestamp": self.timestamp,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "ContextEdge":
        return cls(
            source_id=data["source_id"],
            target_id=data["target_id"],
            type=EdgeType(data["type"]),
            description=data.get("description", ""),
            timestamp=data.get("timestamp", datetime.now().isoformat()),
        )


class ContextManager:
    """ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì"""

    def __init__(self, session_id: Optional[str] = None):
        """
        Args:
            session_id: ì„¸ì…˜ ID (Noneì´ë©´ ìë™ ìƒì„±)
        """
        CONTEXT_DIR.mkdir(parents=True, exist_ok=True)

        self.session_id = session_id or f"ctx_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.nodes: dict[str, ContextNode] = {}
        self.edges: list[ContextEdge] = []
        self._node_counter = 0

        # v13.0: CircuitBreaker ë° Verification ì¶”ê°€
        self.circuit_breaker = CircuitBreaker()
        self.verifications: dict[str, VerificationResult] = {}

        # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        self._load()

    def _load(self):
        """ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        if CONTEXT_FILE.exists():
            try:
                with open(CONTEXT_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)

                    # ê°™ì€ ì„¸ì…˜ë§Œ ë¡œë“œ
                    if data.get("session_id") == self.session_id:
                        for node_data in data.get("nodes", []):
                            node = ContextNode.from_dict(node_data)
                            self.nodes[node.id] = node

                        for edge_data in data.get("edges", []):
                            edge = ContextEdge.from_dict(edge_data)
                            self.edges.append(edge)

                        self._node_counter = data.get("node_counter", len(self.nodes))

                        # v13.0: CircuitBreaker ë¡œë“œ
                        if "circuit_breaker" in data:
                            self.circuit_breaker = CircuitBreaker.from_dict(data["circuit_breaker"])

                        # v13.0: Verifications ë¡œë“œ
                        for vdata in data.get("verifications", []):
                            vr = VerificationResult.from_dict(vdata)
                            self.verifications[vr.check_type] = vr
            except (json.JSONDecodeError, OSError):
                pass

    def _save(self):
        """ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (ìë™ ì •ë¦¬ í¬í•¨)"""
        # ì €ì¥ ì „ ìë™ ì •ë¦¬ ì²´í¬
        self._auto_cleanup()

        try:
            with open(CONTEXT_FILE, "w", encoding="utf-8") as f:
                json.dump({
                    "session_id": self.session_id,
                    "updated_at": datetime.now().isoformat(),
                    "node_counter": self._node_counter,
                    "nodes": [n.to_dict() for n in self.nodes.values()],
                    "edges": [e.to_dict() for e in self.edges],
                    # v13.0: CircuitBreaker ì €ì¥
                    "circuit_breaker": self.circuit_breaker.to_dict(),
                    # v13.0: Verifications ì €ì¥
                    "verifications": [v.to_dict() for v in self.verifications.values()],
                }, f, ensure_ascii=False, indent=2)
        except OSError as e:
            print(f"Warning: Failed to save context: {e}")

    # === ìë™ ì •ë¦¬ ë©”ì»¤ë‹ˆì¦˜ ===

    def _auto_cleanup(self):
        """ìë™ ì •ë¦¬ íŠ¸ë¦¬ê±° (ë…¸ë“œ ìˆ˜, íŒŒì¼ í¬ê¸°, TTL ê¸°ë°˜)"""
        cleanup_needed = False
        reason = ""

        # 1. ë…¸ë“œ ìˆ˜ ì²´í¬
        node_threshold = int(MAX_NODES * CLEANUP_THRESHOLD_PERCENT / 100)
        if len(self.nodes) > node_threshold:
            cleanup_needed = True
            reason = f"ë…¸ë“œ ìˆ˜ ì´ˆê³¼ ({len(self.nodes)}/{MAX_NODES})"

        # 2. ì—£ì§€ ìˆ˜ ì²´í¬
        edge_threshold = int(MAX_EDGES * CLEANUP_THRESHOLD_PERCENT / 100)
        if len(self.edges) > edge_threshold:
            cleanup_needed = True
            reason = f"ì—£ì§€ ìˆ˜ ì´ˆê³¼ ({len(self.edges)}/{MAX_EDGES})"

        # 3. íŒŒì¼ í¬ê¸° ì²´í¬
        if CONTEXT_FILE.exists():
            file_size_kb = CONTEXT_FILE.stat().st_size / 1024
            size_threshold = MAX_FILE_SIZE_KB * CLEANUP_THRESHOLD_PERCENT / 100
            if file_size_kb > size_threshold:
                cleanup_needed = True
                reason = f"íŒŒì¼ í¬ê¸° ì´ˆê³¼ ({file_size_kb:.1f}KB/{MAX_FILE_SIZE_KB}KB)"

        if cleanup_needed:
            self._perform_cleanup(reason)

    def _perform_cleanup(self, reason: str = ""):
        """ì •ë¦¬ ìˆ˜í–‰ (ì¤‘ìš”ë„ + TTL ê¸°ë°˜)"""
        now = datetime.now()
        ttl_cutoff = now - timedelta(hours=TTL_HOURS)

        # ì‚­ì œ ëŒ€ìƒ ë…¸ë“œ ìˆ˜ì§‘
        nodes_to_remove = []

        for node_id, node in self.nodes.items():
            # TTL ë§Œë£Œ ì²´í¬
            try:
                node_time = datetime.fromisoformat(node.timestamp)
                is_expired = node_time < ttl_cutoff
            except (ValueError, TypeError):
                is_expired = False

            # ì‚­ì œ ì¡°ê±´: (TTL ë§Œë£Œ AND LOW) OR (TTL ë§Œë£Œ AND MEDIUM AND ì •ë¦¬ í•„ìš”)
            if is_expired:
                if node.importance == Importance.LOW:
                    nodes_to_remove.append((node_id, node, "TTL+LOW"))
                elif node.importance == Importance.MEDIUM and len(self.nodes) > MAX_NODES:
                    nodes_to_remove.append((node_id, node, "TTL+MEDIUM+OVERFLOW"))

        # ì•„ì§ MAXë¥¼ ì´ˆê³¼í•˜ë©´ LOW importance ìš°ì„  ì‚­ì œ (TTL ë¬´ê´€)
        if len(self.nodes) - len(nodes_to_remove) > MAX_NODES:
            low_nodes = [
                (nid, n, "LOW_PRIORITY")
                for nid, n in self.nodes.items()
                if n.importance == Importance.LOW and nid not in [x[0] for x in nodes_to_remove]
            ]
            # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì •ë ¬
            low_nodes.sort(key=lambda x: x[1].timestamp)
            nodes_to_remove.extend(low_nodes[:ARCHIVE_BATCH_SIZE])

        if nodes_to_remove:
            # ì•„ì¹´ì´ë¸Œ í›„ ì‚­ì œ
            self._archive_nodes([n for _, n, _ in nodes_to_remove], reason)

            # ë…¸ë“œ ì‚­ì œ
            removed_ids = set()
            for node_id, _, _ in nodes_to_remove:
                if node_id in self.nodes:
                    del self.nodes[node_id]
                    removed_ids.add(node_id)

            # ê´€ë ¨ ì—£ì§€ ì‚­ì œ
            self.edges = [
                e for e in self.edges
                if e.source_id not in removed_ids and e.target_id not in removed_ids
            ]

    def _archive_nodes(self, nodes: list, reason: str = ""):
        """ì‚­ì œë  ë…¸ë“œë¥¼ ì•„ì¹´ì´ë¸Œ íŒŒì¼ë¡œ ë³´ê´€"""
        if not nodes:
            return

        ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)

        archive_file = ARCHIVE_DIR / f"archive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(archive_file, "w", encoding="utf-8") as f:
                json.dump({
                    "session_id": self.session_id,
                    "archived_at": datetime.now().isoformat(),
                    "reason": reason,
                    "node_count": len(nodes),
                    "nodes": [n.to_dict() for n in nodes],
                }, f, ensure_ascii=False, indent=2)
        except OSError:
            pass  # ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨í•´ë„ ì •ë¦¬ëŠ” ì§„í–‰

    def force_cleanup(self, keep_high_only: bool = False) -> dict:
        """
        ê°•ì œ ì •ë¦¬ ìˆ˜í–‰

        Args:
            keep_high_only: Trueë©´ HIGH importanceë§Œ ë³´ì¡´

        Returns:
            ì •ë¦¬ ê²°ê³¼ í†µê³„
        """
        before_nodes = len(self.nodes)
        before_edges = len(self.edges)

        nodes_to_remove = []

        for node_id, node in self.nodes.items():
            if keep_high_only:
                if node.importance != Importance.HIGH:
                    nodes_to_remove.append((node_id, node))
            else:
                if node.importance == Importance.LOW:
                    nodes_to_remove.append((node_id, node))

        if nodes_to_remove:
            self._archive_nodes([n for _, n in nodes_to_remove], "force_cleanup")

            removed_ids = set()
            for node_id, _ in nodes_to_remove:
                if node_id in self.nodes:
                    del self.nodes[node_id]
                    removed_ids.add(node_id)

            self.edges = [
                e for e in self.edges
                if e.source_id not in removed_ids and e.target_id not in removed_ids
            ]

        # ê°•ì œ ì •ë¦¬ í›„ ì €ì¥ (ì¬ê·€ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì €ì¥)
        try:
            with open(CONTEXT_FILE, "w", encoding="utf-8") as f:
                json.dump({
                    "session_id": self.session_id,
                    "updated_at": datetime.now().isoformat(),
                    "node_counter": self._node_counter,
                    "nodes": [n.to_dict() for n in self.nodes.values()],
                    "edges": [e.to_dict() for e in self.edges],
                }, f, ensure_ascii=False, indent=2)
        except OSError:
            pass

        return {
            "before_nodes": before_nodes,
            "after_nodes": len(self.nodes),
            "removed_nodes": before_nodes - len(self.nodes),
            "before_edges": before_edges,
            "after_edges": len(self.edges),
            "removed_edges": before_edges - len(self.edges),
        }

    def get_file_stats(self) -> dict:
        """íŒŒì¼ í¬ê¸° ë° ì œí•œ ìƒíƒœ ì¡°íšŒ"""
        file_size_kb = 0
        if CONTEXT_FILE.exists():
            file_size_kb = CONTEXT_FILE.stat().st_size / 1024

        return {
            "file_size_kb": round(file_size_kb, 2),
            "max_file_size_kb": MAX_FILE_SIZE_KB,
            "file_usage_percent": round(file_size_kb / MAX_FILE_SIZE_KB * 100, 1),
            "node_count": len(self.nodes),
            "max_nodes": MAX_NODES,
            "node_usage_percent": round(len(self.nodes) / MAX_NODES * 100, 1),
            "edge_count": len(self.edges),
            "max_edges": MAX_EDGES,
            "edge_usage_percent": round(len(self.edges) / MAX_EDGES * 100, 1),
            "ttl_hours": TTL_HOURS,
            "cleanup_threshold_percent": CLEANUP_THRESHOLD_PERCENT,
        }

    def _generate_id(self, prefix: str) -> str:
        """ê³ ìœ  ID ìƒì„±"""
        self._node_counter += 1
        return f"{prefix}_{self._node_counter}"

    # === ë…¸ë“œ ì¶”ê°€ API ===

    def record_request(self, content: str, metadata: Optional[dict] = None) -> str:
        """ì‚¬ìš©ì ìš”ì²­ ê¸°ë¡"""
        node_id = self._generate_id("req")
        node = ContextNode(
            id=node_id,
            type=NodeType.REQUEST,
            content=content,
            importance=Importance.HIGH,  # ìš”ì²­ì€ í•­ìƒ HIGH
            metadata=metadata or {},
        )
        self.nodes[node_id] = node
        self._save()
        return node_id

    def record_analysis(
        self,
        content: str,
        caused_by: Optional[str] = None,
        importance: Importance = Importance.MEDIUM,
        metadata: Optional[dict] = None
    ) -> str:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë¡"""
        node_id = self._generate_id("analysis")
        node = ContextNode(
            id=node_id,
            type=NodeType.ANALYSIS,
            content=content,
            importance=importance,
            metadata=metadata or {},
        )
        self.nodes[node_id] = node

        if caused_by and caused_by in self.nodes:
            self._add_edge(caused_by, node_id, EdgeType.LEADS_TO, "ë¶„ì„ ìˆ˜í–‰")

        self._save()
        return node_id

    def record_decision(
        self,
        content: str,
        rationale: str,
        caused_by: Optional[str] = None,
        alternatives: Optional[list[str]] = None,
        importance: Importance = Importance.HIGH,
        metadata: Optional[dict] = None
    ) -> str:
        """ê²°ì • ì‚¬í•­ ê¸°ë¡"""
        node_id = self._generate_id("decision")
        node = ContextNode(
            id=node_id,
            type=NodeType.DECISION,
            content=content,
            importance=importance,
            metadata={
                "rationale": rationale,
                "alternatives": alternatives or [],
                **(metadata or {}),
            },
        )
        self.nodes[node_id] = node

        if caused_by and caused_by in self.nodes:
            self._add_edge(caused_by, node_id, EdgeType.LEADS_TO, "ê²°ì •ìœ¼ë¡œ ì´ì–´ì§")

        # ê±°ë¶€ëœ ëŒ€ì•ˆ ê¸°ë¡
        if alternatives:
            for alt in alternatives:
                alt_id = self._generate_id("rejected")
                alt_node = ContextNode(
                    id=alt_id,
                    type=NodeType.REJECTED,
                    content=alt,
                    importance=Importance.LOW,
                )
                self.nodes[alt_id] = alt_node
                self._add_edge(node_id, alt_id, EdgeType.REJECTS, "ëŒ€ì•ˆ ê±°ë¶€")

        self._save()
        return node_id

    def record_file_change(
        self,
        file_path: str,
        change_type: str,  # create, modify, delete
        description: str,
        caused_by: Optional[str] = None,
        metadata: Optional[dict] = None
    ) -> str:
        """íŒŒì¼ ë³€ê²½ ê¸°ë¡"""
        node_id = self._generate_id("file")
        node = ContextNode(
            id=node_id,
            type=NodeType.FILE,
            content=f"[{change_type.upper()}] {file_path}: {description}",
            importance=Importance.MEDIUM,
            metadata={
                "file_path": file_path,
                "change_type": change_type,
                **(metadata or {}),
            },
        )
        self.nodes[node_id] = node

        if caused_by and caused_by in self.nodes:
            self._add_edge(caused_by, node_id, EdgeType.CAUSES, f"{change_type} ìœ ë°œ")

        self._save()
        return node_id

    def record_error(
        self,
        error_message: str,
        caused_by: Optional[str] = None,
        context: Optional[str] = None,
        importance: Importance = Importance.HIGH,
        metadata: Optional[dict] = None
    ) -> str:
        """ì—ëŸ¬ ê¸°ë¡"""
        node_id = self._generate_id("error")
        node = ContextNode(
            id=node_id,
            type=NodeType.ERROR,
            content=error_message,
            importance=importance,
            metadata={
                "context": context,
                "resolved": False,
                **(metadata or {}),
            },
        )
        self.nodes[node_id] = node

        if caused_by and caused_by in self.nodes:
            self._add_edge(caused_by, node_id, EdgeType.CAUSES, "ì—ëŸ¬ ìœ ë°œ")

        self._save()
        return node_id

    def record_solution(
        self,
        content: str,
        resolves: str,  # í•´ê²°í•˜ëŠ” ì—ëŸ¬ ë…¸ë“œ ID
        approach: str,
        importance: Importance = Importance.HIGH,
        metadata: Optional[dict] = None
    ) -> str:
        """í•´ê²°ì±… ê¸°ë¡"""
        node_id = self._generate_id("solution")
        node = ContextNode(
            id=node_id,
            type=NodeType.SOLUTION,
            content=content,
            importance=importance,
            metadata={
                "approach": approach,
                **(metadata or {}),
            },
        )
        self.nodes[node_id] = node

        if resolves and resolves in self.nodes:
            self._add_edge(resolves, node_id, EdgeType.RESOLVED_BY, "í•´ê²°ë¨")
            # ì—ëŸ¬ ë…¸ë“œ resolved ìƒíƒœ ì—…ë°ì´íŠ¸
            self.nodes[resolves].metadata["resolved"] = True

        self._save()
        return node_id

    def record_learning(
        self,
        content: str,
        source: Optional[str] = None,
        importance: Importance = Importance.HIGH,
        metadata: Optional[dict] = None
    ) -> str:
        """í•™ìŠµ ë‚´ìš© ê¸°ë¡"""
        node_id = self._generate_id("learning")
        node = ContextNode(
            id=node_id,
            type=NodeType.LEARNING,
            content=content,
            importance=importance,
            metadata=metadata or {},
        )
        self.nodes[node_id] = node

        if source and source in self.nodes:
            self._add_edge(source, node_id, EdgeType.LEADS_TO, "í•™ìŠµìœ¼ë¡œ ì´ì–´ì§")

        self._save()
        return node_id

    def _add_edge(
        self,
        source_id: str,
        target_id: str,
        edge_type: EdgeType,
        description: str = ""
    ):
        """ì—£ì§€ ì¶”ê°€"""
        edge = ContextEdge(
            source_id=source_id,
            target_id=target_id,
            type=edge_type,
            description=description,
        )
        self.edges.append(edge)

    def add_dependency(self, node_id: str, depends_on: str, description: str = ""):
        """ì˜ì¡´ì„± ê´€ê³„ ì¶”ê°€"""
        if node_id in self.nodes and depends_on in self.nodes:
            self._add_edge(node_id, depends_on, EdgeType.DEPENDS_ON, description)
            self._save()

    def add_blocking(self, blocker_id: str, blocked_id: str, description: str = ""):
        """ì°¨ë‹¨ ê´€ê³„ ì¶”ê°€"""
        if blocker_id in self.nodes and blocked_id in self.nodes:
            self._add_edge(blocker_id, blocked_id, EdgeType.BLOCKS, description)
            self._save()

    # === ì¡°íšŒ API ===

    def get_node(self, node_id: str) -> Optional[ContextNode]:
        """ë…¸ë“œ ì¡°íšŒ"""
        return self.nodes.get(node_id)

    def get_nodes_by_type(self, node_type: NodeType) -> list[ContextNode]:
        """ìœ í˜•ë³„ ë…¸ë“œ ì¡°íšŒ"""
        return [n for n in self.nodes.values() if n.type == node_type]

    def get_high_importance_nodes(self) -> list[ContextNode]:
        """HIGH ì¤‘ìš”ë„ ë…¸ë“œ ì¡°íšŒ"""
        return [n for n in self.nodes.values() if n.importance == Importance.HIGH]

    def get_unresolved_errors(self) -> list[ContextNode]:
        """ë¯¸í•´ê²° ì—ëŸ¬ ì¡°íšŒ"""
        return [
            n for n in self.nodes.values()
            if n.type == NodeType.ERROR and not n.metadata.get("resolved", False)
        ]

    def get_related_nodes(self, node_id: str) -> dict[str, list[ContextNode]]:
        """ë…¸ë“œì™€ ê´€ë ¨ëœ ëª¨ë“  ë…¸ë“œ ì¡°íšŒ"""
        if node_id not in self.nodes:
            return {}

        result = {
            "causes": [],      # ì´ ë…¸ë“œê°€ ìœ ë°œí•œ ê²ƒë“¤
            "caused_by": [],   # ì´ ë…¸ë“œë¥¼ ìœ ë°œí•œ ê²ƒë“¤
            "resolves": [],    # ì´ ë…¸ë“œê°€ í•´ê²°í•œ ê²ƒë“¤
            "resolved_by": [], # ì´ ë…¸ë“œë¥¼ í•´ê²°í•œ ê²ƒë“¤
            "blocks": [],      # ì´ ë…¸ë“œê°€ ì°¨ë‹¨í•˜ëŠ” ê²ƒë“¤
            "blocked_by": [],  # ì´ ë…¸ë“œë¥¼ ì°¨ë‹¨í•˜ëŠ” ê²ƒë“¤
            "depends_on": [],  # ì´ ë…¸ë“œê°€ ì˜ì¡´í•˜ëŠ” ê²ƒë“¤
        }

        for edge in self.edges:
            if edge.source_id == node_id:
                target_node = self.nodes.get(edge.target_id)
                if target_node:
                    if edge.type == EdgeType.CAUSES:
                        result["causes"].append(target_node)
                    elif edge.type == EdgeType.RESOLVED_BY:
                        result["resolved_by"].append(target_node)
                    elif edge.type == EdgeType.BLOCKS:
                        result["blocks"].append(target_node)
                    elif edge.type == EdgeType.DEPENDS_ON:
                        result["depends_on"].append(target_node)

            elif edge.target_id == node_id:
                source_node = self.nodes.get(edge.source_id)
                if source_node:
                    if edge.type == EdgeType.CAUSES:
                        result["caused_by"].append(source_node)
                    elif edge.type == EdgeType.RESOLVED_BY:
                        result["resolves"].append(source_node)
                    elif edge.type == EdgeType.BLOCKS:
                        result["blocked_by"].append(source_node)

        return result

    # === Compact Summary ìƒì„± ===

    def generate_compact_summary(self) -> str:
        """Compaction ìƒì¡´ìš© ì••ì¶• ìš”ì•½ ìƒì„±"""
        summary_lines = [
            "# Context Compact Summary",
            f"Session: {self.session_id}",
            f"Generated: {datetime.now().isoformat()}",
            "",
        ]

        # 1. ì›ë³¸ ìš”ì²­
        requests = self.get_nodes_by_type(NodeType.REQUEST)
        if requests:
            summary_lines.append("## Original Request")
            for req in requests:
                summary_lines.append(f"- {req.content}")
            summary_lines.append("")

        # 2. í•µì‹¬ ê²°ì • (HIGH importance)
        decisions = [
            n for n in self.get_nodes_by_type(NodeType.DECISION)
            if n.importance == Importance.HIGH
        ]
        if decisions:
            summary_lines.append("## Key Decisions")
            for dec in decisions:
                summary_lines.append(f"- **{dec.content}**")
                if dec.metadata.get("rationale"):
                    summary_lines.append(f"  - Rationale: {dec.metadata['rationale']}")
            summary_lines.append("")

        # 3. ë³€ê²½ëœ íŒŒì¼
        files = self.get_nodes_by_type(NodeType.FILE)
        if files:
            summary_lines.append("## Changed Files")
            for f in files:
                summary_lines.append(f"- {f.content}")
            summary_lines.append("")

        # 4. ë¯¸í•´ê²° ì—ëŸ¬
        unresolved = self.get_unresolved_errors()
        if unresolved:
            summary_lines.append("## Unresolved Errors")
            for err in unresolved:
                summary_lines.append(f"- âš ï¸ {err.content}")
            summary_lines.append("")

        # 5. í•´ê²°ëœ ì—ëŸ¬ì™€ ì†”ë£¨ì…˜
        solutions = self.get_nodes_by_type(NodeType.SOLUTION)
        if solutions:
            summary_lines.append("## Solutions Applied")
            for sol in solutions:
                summary_lines.append(f"- âœ… {sol.content}")
                if sol.metadata.get("approach"):
                    summary_lines.append(f"  - Approach: {sol.metadata['approach']}")
            summary_lines.append("")

        # 6. í•™ìŠµ ë‚´ìš© (íŒ¨í„´, ì¸ì‚¬ì´íŠ¸)
        learnings = self.get_nodes_by_type(NodeType.LEARNING)
        if learnings:
            summary_lines.append("## Learnings")
            for learn in learnings:
                summary_lines.append(f"- ğŸ’¡ {learn.content}")
            summary_lines.append("")

        # 7. ì¸ê³¼ê´€ê³„ ìš”ì•½
        causal_chains = self._extract_causal_chains()
        if causal_chains:
            summary_lines.append("## Causality Chains")
            for chain in causal_chains[:5]:  # ìƒìœ„ 5ê°œë§Œ
                summary_lines.append(f"- {chain}")
            summary_lines.append("")

        summary = "\n".join(summary_lines)

        # íŒŒì¼ë¡œ ì €ì¥
        try:
            with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
                f.write(summary)
        except OSError:
            pass

        return summary

    def _extract_causal_chains(self) -> list[str]:
        """ì¸ê³¼ê´€ê³„ ì²´ì¸ ì¶”ì¶œ"""
        chains = []

        # REQUEST â†’ ... â†’ SOLUTION ë˜ëŠ” DECISION ì²´ì¸ ì°¾ê¸°
        requests = self.get_nodes_by_type(NodeType.REQUEST)

        for req in requests:
            chain = self._trace_chain(req.id, set())
            if len(chain) > 1:
                chain_str = " â†’ ".join([
                    f"[{self.nodes[nid].type.value}] {self.nodes[nid].content[:30]}..."
                    if len(self.nodes[nid].content) > 30
                    else f"[{self.nodes[nid].type.value}] {self.nodes[nid].content}"
                    for nid in chain
                ])
                chains.append(chain_str)

        return chains

    def _trace_chain(self, start_id: str, visited: set) -> list[str]:
        """ë…¸ë“œì—ì„œ ì‹œì‘í•˜ëŠ” ì²´ì¸ ì¶”ì """
        if start_id in visited:
            return []

        visited.add(start_id)
        chain = [start_id]

        # ë‹¤ìŒ ë…¸ë“œ ì°¾ê¸° (LEADS_TO, CAUSES, RESOLVED_BY)
        for edge in self.edges:
            if edge.source_id == start_id and edge.type in [
                EdgeType.LEADS_TO, EdgeType.CAUSES, EdgeType.RESOLVED_BY
            ]:
                next_chain = self._trace_chain(edge.target_id, visited)
                if next_chain:
                    chain.extend(next_chain)
                    break  # ì²« ë²ˆì§¸ ì²´ì¸ë§Œ

        return chain

    def get_stats(self) -> dict:
        """í†µê³„ ì¡°íšŒ"""
        type_counts = {}
        for node in self.nodes.values():
            type_counts[node.type.value] = type_counts.get(node.type.value, 0) + 1

        importance_counts = {}
        for node in self.nodes.values():
            importance_counts[node.importance.value] = importance_counts.get(node.importance.value, 0) + 1

        return {
            "session_id": self.session_id,
            "total_nodes": len(self.nodes),
            "total_edges": len(self.edges),
            "nodes_by_type": type_counts,
            "nodes_by_importance": importance_counts,
            "unresolved_errors": len(self.get_unresolved_errors()),
            "high_importance": len(self.get_high_importance_nodes()),
        }

    def clear(self):
        """ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        self.nodes = {}
        self.edges = []
        self._node_counter = 0
        self._save()

    # === ì„¸ì…˜ ë³µì› API ===

    def get_restoration_prompt(self) -> str:
        """
        /clear í›„ ë³µì›ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±

        HIGH importance ë…¸ë“œì™€ ì¸ê³¼ê´€ê³„ ì²´ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ
        ìƒˆ ì„¸ì…˜ì—ì„œ ë§¥ë½ì„ ë³µì›í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Returns:
            ë³µì›ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        if not self.nodes:
            return ""

        lines = [
            "# ì´ì „ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë³µì›",
            "",
            "ë‹¤ìŒì€ ì´ì „ ì„¸ì…˜ì—ì„œ ì €ì¥ëœ í•µì‹¬ ë§¥ë½ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—…ì„ ê³„ì†í•˜ì„¸ìš”.",
            "",
        ]

        # 1. ì›ë³¸ ìš”ì²­
        requests = self.get_nodes_by_type(NodeType.REQUEST)
        if requests:
            lines.append("## ì›ë³¸ ìš”ì²­")
            for req in requests:
                lines.append(f"- {req.content}")
            lines.append("")

        # 2. í•µì‹¬ ê²°ì •ì‚¬í•­ (HIGH importance)
        decisions = [
            n for n in self.get_nodes_by_type(NodeType.DECISION)
            if n.importance == Importance.HIGH
        ]
        if decisions:
            lines.append("## í•µì‹¬ ê²°ì •ì‚¬í•­")
            for dec in decisions:
                lines.append(f"- **{dec.content}**")
                if dec.metadata.get("rationale"):
                    lines.append(f"  - ê·¼ê±°: {dec.metadata['rationale']}")
                if dec.metadata.get("alternatives"):
                    alts = ", ".join(dec.metadata["alternatives"])
                    lines.append(f"  - ê±°ë¶€ëœ ëŒ€ì•ˆ: {alts}")
            lines.append("")

        # 3. ë³€ê²½ëœ íŒŒì¼
        files = self.get_nodes_by_type(NodeType.FILE)
        if files:
            lines.append("## ë³€ê²½ëœ íŒŒì¼")
            for f in files:
                lines.append(f"- {f.content}")
            lines.append("")

        # 4. í•´ê²°ëœ ì—ëŸ¬ì™€ ì†”ë£¨ì…˜
        solutions = self.get_nodes_by_type(NodeType.SOLUTION)
        if solutions:
            lines.append("## ì ìš©ëœ ì†”ë£¨ì…˜")
            for sol in solutions:
                lines.append(f"- âœ… {sol.content}")
                if sol.metadata.get("approach"):
                    lines.append(f"  - ì ‘ê·¼ë²•: {sol.metadata['approach']}")
            lines.append("")

        # 5. ë¯¸í•´ê²° ì—ëŸ¬ (ìˆë‹¤ë©´)
        unresolved = self.get_unresolved_errors()
        if unresolved:
            lines.append("## âš ï¸ ë¯¸í•´ê²° ì—ëŸ¬")
            for err in unresolved:
                lines.append(f"- {err.content}")
                if err.metadata.get("context"):
                    lines.append(f"  - ìƒí™©: {err.metadata['context']}")
            lines.append("")

        # 6. í•™ìŠµ ë‚´ìš©
        learnings = self.get_nodes_by_type(NodeType.LEARNING)
        if learnings:
            lines.append("## í•™ìŠµ ë‚´ìš©")
            for learn in learnings:
                lines.append(f"- ğŸ’¡ {learn.content}")
            lines.append("")

        # 7. ì¸ê³¼ê´€ê³„ ì²´ì¸ ìš”ì•½
        causal_chains = self._extract_causal_chains()
        if causal_chains:
            lines.append("## ì¸ê³¼ê´€ê³„ íë¦„")
            for chain in causal_chains[:3]:  # ìƒìœ„ 3ê°œ
                lines.append(f"- {chain}")
            lines.append("")

        # 8. ë‹¤ìŒ ì‘ì—… íŒíŠ¸
        lines.append("## ë‹¤ìŒ ì‘ì—…")
        if unresolved:
            lines.append("- ìœ„ì˜ ë¯¸í•´ê²° ì—ëŸ¬ë¥¼ ë¨¼ì € í•´ê²°í•˜ì„¸ìš”")
        else:
            lines.append("- ì´ì „ ì„¸ì…˜ì˜ ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—…ì„ ê³„ì†í•˜ì„¸ìš”")
        lines.append("")

        return "\n".join(lines)

    def get_session_info(self) -> dict:
        """
        ì„¸ì…˜ ë³µì› ì •ë³´ ì¡°íšŒ

        Returns:
            {
                "has_context": bool,
                "session_id": str,
                "node_count": int,
                "high_importance_count": int,
                "unresolved_errors": int,
                "last_updated": str,
                "requests": list[str],
                "decisions": list[str],
            }
        """
        if not self.nodes:
            return {"has_context": False}

        requests = [n.content for n in self.get_nodes_by_type(NodeType.REQUEST)]
        decisions = [
            n.content for n in self.get_nodes_by_type(NodeType.DECISION)
            if n.importance == Importance.HIGH
        ]

        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì°¾ê¸°
        last_updated = ""
        if self.nodes:
            timestamps = [n.timestamp for n in self.nodes.values()]
            last_updated = max(timestamps) if timestamps else ""

        return {
            "has_context": True,
            "session_id": self.session_id,
            "node_count": len(self.nodes),
            "high_importance_count": len(self.get_high_importance_nodes()),
            "unresolved_errors": len(self.get_unresolved_errors()),
            "last_updated": last_updated,
            "requests": requests,
            "decisions": decisions[:5],  # ìµœëŒ€ 5ê°œ
        }

    def should_restore(self) -> bool:
        """
        ë³µì›ì´ ì˜ë¯¸ìˆëŠ”ì§€ íŒë‹¨

        Returns:
            ë³µì› ê¶Œì¥ ì—¬ë¶€
        """
        if not self.nodes:
            return False

        # HIGH importance ë…¸ë“œê°€ 3ê°œ ì´ìƒì´ë©´ ë³µì› ê¶Œì¥
        high_count = len(self.get_high_importance_nodes())
        if high_count >= 3:
            return True

        # ë¯¸í•´ê²° ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ë³µì› ê¶Œì¥
        if self.get_unresolved_errors():
            return True

        # ê²°ì •ì‚¬í•­ì´ ìˆìœ¼ë©´ ë³µì› ê¶Œì¥
        if self.get_nodes_by_type(NodeType.DECISION):
            return True

        return False

    # === v13.0: CircuitBreaker í†µí•© API ===

    def record_failure(self, task_key: str, error_node_id: str) -> str:
        """
        ì‹¤íŒ¨ ê¸°ë¡ ë° ë‹¤ìŒ ì•¡ì…˜ ë°˜í™˜

        Args:
            task_key: íƒœìŠ¤í¬ ì‹ë³„ì
            error_node_id: ê´€ë ¨ ì—ëŸ¬ ë…¸ë“œ ID

        Returns:
            "RETRY" | "ALTERNATE_APPROACH" | "ESCALATE_TO_ARCHITECT"
        """
        # ì—ëŸ¬ ë…¸ë“œì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ
        error_msg = ""
        if error_node_id in self.nodes:
            error_msg = self.nodes[error_node_id].content

        action = self.circuit_breaker.record_failure(task_key, error_msg)
        self._save()
        return action

    def record_success(self, task_key: str):
        """
        ì„±ê³µ ì‹œ CircuitBreaker ë¦¬ì…‹

        Args:
            task_key: íƒœìŠ¤í¬ ì‹ë³„ì
        """
        self.circuit_breaker.reset(task_key)
        self._save()

    def get_escalation_summary(self) -> str:
        """
        Architect ì—ìŠ¤ì»¬ë ˆì´ì…˜ìš© ìš”ì•½ ìƒì„±

        ì—ìŠ¤ì»¬ë ˆì´ì…˜ì´ í•„ìš”í•œ íƒœìŠ¤í¬ë“¤ì˜ ì‹¤íŒ¨ ì´ë ¥ì„ ìš”ì•½í•©ë‹ˆë‹¤.

        Returns:
            ì—ìŠ¤ì»¬ë ˆì´ì…˜ ìš”ì•½ ë¬¸ìì—´
        """
        lines = [
            "# Escalation Summary",
            f"Session: {self.session_id}",
            f"Generated: {datetime.now().isoformat()}",
            "",
        ]

        all_failures = self.circuit_breaker.get_all_failures()
        escalated = {
            k: v for k, v in all_failures.items()
            if v["recommendation"] == "ESCALATE_TO_ARCHITECT"
        }

        if not escalated:
            lines.append("No tasks require escalation.")
            return "\n".join(lines)

        lines.append(f"## Escalated Tasks ({len(escalated)} tasks)")
        lines.append("")

        for task_key, summary in escalated.items():
            lines.append(f"### Task: {task_key}")
            lines.append(f"- **Failure Count**: {summary['failure_count']}")
            lines.append(f"- **Errors**:")

            for err in summary["errors"]:
                lines.append(f"  - [{err['attempt']}] {err['timestamp'][:19]}: {err['error'][:100]}")

            lines.append("")

        # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        unresolved = self.get_unresolved_errors()
        if unresolved:
            lines.append("## Related Unresolved Errors")
            for err in unresolved[:5]:
                lines.append(f"- {err.content[:150]}")
            lines.append("")

        return "\n".join(lines)

    # === v13.0: Verification Protocol í†µí•© API ===

    def record_verification(self, check_type: str, passed: bool, evidence: str) -> str:
        """
        ê²€ì¦ ê²°ê³¼ ê¸°ë¡ (5ë¶„ freshness ì¶”ì )

        Args:
            check_type: ê²€ì¦ ìœ í˜• ("BUILD", "TEST", "LINT", "FUNCTIONALITY", "ARCHITECT", "TODO", "ERROR_FREE")
            passed: í†µê³¼ ì—¬ë¶€
            evidence: ì¦ê±° (ì¶œë ¥ ë¡œê·¸ ë“±)

        Returns:
            ê²€ì¦ ê²°ê³¼ ë…¸ë“œ ID
        """
        vr = VerificationResult(
            check_type=check_type,
            passed=passed,
            evidence=evidence,
        )
        self.verifications[check_type] = vr
        self._save()

        # ì»¨í…ìŠ¤íŠ¸ ë…¸ë“œë¡œë„ ê¸°ë¡ (ì¶”ì ìš©)
        status = "PASSED" if passed else "FAILED"
        node_id = self._generate_id("verify")
        node = ContextNode(
            id=node_id,
            type=NodeType.ANALYSIS,
            content=f"[VERIFICATION] {check_type}: {status}",
            importance=Importance.MEDIUM if passed else Importance.HIGH,
            metadata={
                "check_type": check_type,
                "passed": passed,
                "evidence": evidence[:500] if len(evidence) > 500 else evidence,
            },
        )
        self.nodes[node_id] = node
        self._save()

        return node_id

    def get_verification_status(self) -> dict:
        """
        ëª¨ë“  ê²€ì¦ ìƒíƒœ ì¡°íšŒ

        Returns:
            {
                "BUILD": {"passed": True, "fresh": True, "age_seconds": 120},
                ...
            }
        """
        now = datetime.now()
        result = {}

        for check_type, vr in self.verifications.items():
            try:
                ts = datetime.fromisoformat(vr.timestamp)
                age_seconds = (now - ts).total_seconds()
                fresh = age_seconds <= VERIFICATION_FRESHNESS_SECONDS
            except (ValueError, TypeError):
                age_seconds = float('inf')
                fresh = False

            result[check_type] = {
                "passed": vr.passed,
                "fresh": fresh,
                "age_seconds": int(age_seconds),
                "timestamp": vr.timestamp,
                "evidence": vr.evidence[:200] if len(vr.evidence) > 200 else vr.evidence,
            }

        return result

    def is_verification_fresh(self, check_type: str, max_age_seconds: int = VERIFICATION_FRESHNESS_SECONDS) -> bool:
        """
        ê²€ì¦ ê²°ê³¼ê°€ freshí•œì§€ í™•ì¸

        Args:
            check_type: ê²€ì¦ ìœ í˜•
            max_age_seconds: ìµœëŒ€ ìœ íš¨ ì‹œê°„ (ê¸°ë³¸ 5ë¶„)

        Returns:
            fresh ì—¬ë¶€
        """
        if check_type not in self.verifications:
            return False

        vr = self.verifications[check_type]
        try:
            ts = datetime.fromisoformat(vr.timestamp)
            age_seconds = (datetime.now() - ts).total_seconds()
            return age_seconds <= max_age_seconds
        except (ValueError, TypeError):
            return False

    # === v13.0: Notepad Wisdom ì—°ë™ API ===

    def export_to_wisdom(self, plan_name: str) -> dict:
        """
        HIGH importance ë…¸ë“œë¥¼ Notepad Wisdomìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            plan_name: ê³„íš ì´ë¦„ (ë””ë ‰í† ë¦¬ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©)

        Returns:
            {"learnings": int, "decisions": int, "issues": int}
        """
        wisdom_path = WISDOM_DIR / plan_name
        wisdom_path.mkdir(parents=True, exist_ok=True)

        counts = {"learnings": 0, "decisions": 0, "issues": 0}

        # 1. Learnings ë‚´ë³´ë‚´ê¸°
        learnings = [
            n for n in self.get_nodes_by_type(NodeType.LEARNING)
            if n.importance == Importance.HIGH
        ]
        if learnings:
            learning_file = wisdom_path / "learnings.md"
            lines = [
                "# Learnings",
                f"Exported from session: {self.session_id}",
                f"Exported at: {datetime.now().isoformat()}",
                "",
            ]
            for learn in learnings:
                lines.append(f"## {learn.timestamp[:10]}")
                lines.append(f"- {learn.content}")
                if learn.metadata:
                    for k, v in learn.metadata.items():
                        if k not in ("timestamp",):
                            lines.append(f"  - {k}: {v}")
                lines.append("")

            try:
                with open(learning_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(lines) + "\n")
                counts["learnings"] = len(learnings)
            except OSError:
                pass

        # 2. Decisions ë‚´ë³´ë‚´ê¸°
        decisions = [
            n for n in self.get_nodes_by_type(NodeType.DECISION)
            if n.importance == Importance.HIGH
        ]
        if decisions:
            decision_file = wisdom_path / "decisions.md"
            lines = [
                "# Decisions",
                f"Exported from session: {self.session_id}",
                f"Exported at: {datetime.now().isoformat()}",
                "",
            ]
            for dec in decisions:
                lines.append(f"## {dec.timestamp[:10]}: {dec.content[:50]}")
                lines.append(f"**Decision**: {dec.content}")
                if dec.metadata.get("rationale"):
                    lines.append(f"**Rationale**: {dec.metadata['rationale']}")
                if dec.metadata.get("alternatives"):
                    lines.append(f"**Rejected Alternatives**: {', '.join(dec.metadata['alternatives'])}")
                lines.append("")

            try:
                with open(decision_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(lines) + "\n")
                counts["decisions"] = len(decisions)
            except OSError:
                pass

        # 3. Issues ë‚´ë³´ë‚´ê¸° (ERROR + SOLUTION ìŒ)
        errors = [
            n for n in self.get_nodes_by_type(NodeType.ERROR)
            if n.importance == Importance.HIGH or n.metadata.get("resolved")
        ]
        solutions = {
            edge.source_id: self.nodes.get(edge.target_id)
            for edge in self.edges
            if edge.type == EdgeType.RESOLVED_BY
        }

        if errors:
            issue_file = wisdom_path / "issues.md"
            lines = [
                "# Issues",
                f"Exported from session: {self.session_id}",
                f"Exported at: {datetime.now().isoformat()}",
                "",
            ]
            for err in errors:
                lines.append(f"## {err.timestamp[:10]}: Issue")
                lines.append(f"**Error**: {err.content}")
                if err.metadata.get("context"):
                    lines.append(f"**Context**: {err.metadata['context']}")

                sol = solutions.get(err.id)
                if sol:
                    lines.append(f"**Solution**: {sol.content}")
                    if sol.metadata.get("approach"):
                        lines.append(f"**Approach**: {sol.metadata['approach']}")
                else:
                    lines.append("**Status**: Unresolved")
                lines.append("")

            try:
                with open(issue_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(lines) + "\n")
                counts["issues"] = len(errors)
            except OSError:
                pass

        return counts


# === í¸ì˜ í•¨ìˆ˜ ===

def get_current_context() -> Optional[ContextManager]:
    """í˜„ì¬ ì„¸ì…˜ì˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ë¡œë“œ"""
    if CONTEXT_FILE.exists():
        try:
            with open(CONTEXT_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                session_id = data.get("session_id")
                if session_id:
                    return ContextManager(session_id)
        except (json.JSONDecodeError, OSError):
            pass
    return None


def create_context(session_id: Optional[str] = None) -> ContextManager:
    """ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    return ContextManager(session_id)


def get_compact_summary() -> str:
    """í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ì˜ ì••ì¶• ìš”ì•½ ì¡°íšŒ"""
    ctx = get_current_context()
    if ctx:
        return ctx.generate_compact_summary()
    return "No context available"


def check_restorable_session() -> dict:
    """
    ë³µì› ê°€ëŠ¥í•œ ì´ì „ ì„¸ì…˜ í™•ì¸

    Returns:
        {
            "restorable": bool,
            "session_info": dict,
            "restoration_prompt": str (restorableì¼ ë•Œë§Œ)
        }
    """
    # 1. context_graph.json í™•ì¸
    if not CONTEXT_FILE.exists():
        return {"restorable": False, "reason": "No previous context file"}

    ctx = get_current_context()
    if not ctx:
        return {"restorable": False, "reason": "Failed to load context"}

    session_info = ctx.get_session_info()
    if not session_info.get("has_context"):
        return {"restorable": False, "reason": "Empty context"}

    if not ctx.should_restore():
        return {
            "restorable": False,
            "reason": "Not enough important context to restore",
            "session_info": session_info,
        }

    return {
        "restorable": True,
        "session_info": session_info,
        "restoration_prompt": ctx.get_restoration_prompt(),
    }


def get_restoration_prompt() -> str:
    """
    ì´ì „ ì„¸ì…˜ ë³µì›ìš© í”„ë¡¬í”„íŠ¸ ì¡°íšŒ

    /auto ì‹œì‘ ì‹œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì´ì „ ë§¥ë½ì„ ì£¼ì…í•©ë‹ˆë‹¤.

    Returns:
        ë³µì› í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
    """
    ctx = get_current_context()
    if ctx and ctx.should_restore():
        return ctx.get_restoration_prompt()
    return ""


def format_session_summary_for_display() -> str:
    """
    ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì„¸ì…˜ ìš”ì•½ í¬ë§·íŒ…

    Returns:
        í¬ë§·íŒ…ëœ ì„¸ì…˜ ìš”ì•½ ë¬¸ìì—´
    """
    result = check_restorable_session()

    if not result.get("restorable"):
        return f"ë³µì› ê°€ëŠ¥í•œ ì´ì „ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ({result.get('reason', 'Unknown')})"

    info = result["session_info"]

    lines = [
        "## ì´ì „ ì„¸ì…˜ ë°œê²¬",
        "",
        f"- **ì„¸ì…˜ ID**: {info.get('session_id', 'Unknown')}",
        f"- **ë…¸ë“œ ìˆ˜**: {info.get('node_count', 0)}ê°œ",
        f"- **ì¤‘ìš” ë…¸ë“œ**: {info.get('high_importance_count', 0)}ê°œ",
        f"- **ë¯¸í•´ê²° ì—ëŸ¬**: {info.get('unresolved_errors', 0)}ê°œ",
        f"- **ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: {info.get('last_updated', 'Unknown')[:19]}",
        "",
    ]

    requests = info.get("requests", [])
    if requests:
        lines.append("### ì›ë³¸ ìš”ì²­")
        for req in requests:
            lines.append(f"- {req}")
        lines.append("")

    decisions = info.get("decisions", [])
    if decisions:
        lines.append("### ì£¼ìš” ê²°ì •ì‚¬í•­")
        for dec in decisions[:3]:
            lines.append(f"- {dec}")
        lines.append("")

    return "\n".join(lines)


if __name__ == "__main__":
    print("=== Context Manager Test ===\n")

    # í…ŒìŠ¤íŠ¸ 1: ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    print("1. ì»¨í…ìŠ¤íŠ¸ ìƒì„±")
    ctx = ContextManager("test_session")
    print(f"   Session ID: {ctx.session_id}")

    # í…ŒìŠ¤íŠ¸ 2: ìš”ì²­ ê¸°ë¡
    print("\n2. ìš”ì²­ ê¸°ë¡")
    req_id = ctx.record_request("API ì¸ì¦ ê¸°ëŠ¥ êµ¬í˜„")
    print(f"   Request ID: {req_id}")

    # í…ŒìŠ¤íŠ¸ 3: ë¶„ì„ ê¸°ë¡
    print("\n3. ë¶„ì„ ê¸°ë¡")
    analysis_id = ctx.record_analysis(
        "JWT í† í° ë°©ì‹ì´ ì í•©í•¨. ê¸°ì¡´ ì„¸ì…˜ ê¸°ë°˜ ì¸ì¦ì—ì„œ ì „í™˜ í•„ìš”.",
        caused_by=req_id,
        importance=Importance.HIGH
    )
    print(f"   Analysis ID: {analysis_id}")

    # í…ŒìŠ¤íŠ¸ 4: ê²°ì • ê¸°ë¡
    print("\n4. ê²°ì • ê¸°ë¡")
    decision_id = ctx.record_decision(
        "JWT + Refresh Token ë°©ì‹ ì±„íƒ",
        rationale="í™•ì¥ì„±ê³¼ stateless íŠ¹ì„±ì´ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì— ì í•©",
        caused_by=analysis_id,
        alternatives=["Session Cookie ë°©ì‹", "OAuth 2.0 Only"]
    )
    print(f"   Decision ID: {decision_id}")

    # í…ŒìŠ¤íŠ¸ 5: íŒŒì¼ ë³€ê²½ ê¸°ë¡
    print("\n5. íŒŒì¼ ë³€ê²½ ê¸°ë¡")
    file_id = ctx.record_file_change(
        "src/auth/jwt_handler.py",
        "create",
        "JWT í† í° ìƒì„±/ê²€ì¦ í•¸ë“¤ëŸ¬ êµ¬í˜„",
        caused_by=decision_id
    )
    print(f"   File ID: {file_id}")

    # í…ŒìŠ¤íŠ¸ 6: ì—ëŸ¬ ê¸°ë¡
    print("\n6. ì—ëŸ¬ ê¸°ë¡")
    error_id = ctx.record_error(
        "PyJWT ëª¨ë“ˆ import ì‹¤íŒ¨: ModuleNotFoundError",
        caused_by=file_id,
        context="jwt_handler.py ì‹¤í–‰ ì‹œ"
    )
    print(f"   Error ID: {error_id}")

    # í…ŒìŠ¤íŠ¸ 7: ì†”ë£¨ì…˜ ê¸°ë¡
    print("\n7. ì†”ë£¨ì…˜ ê¸°ë¡")
    solution_id = ctx.record_solution(
        "pip install PyJWT ì‹¤í–‰ìœ¼ë¡œ í•´ê²°",
        resolves=error_id,
        approach="ì˜ì¡´ì„± ì„¤ì¹˜"
    )
    print(f"   Solution ID: {solution_id}")

    # í…ŒìŠ¤íŠ¸ 8: í•™ìŠµ ê¸°ë¡
    print("\n8. í•™ìŠµ ê¸°ë¡")
    learning_id = ctx.record_learning(
        "JWT ê´€ë ¨ ì˜ì¡´ì„±ì€ requirements.txtì— ì¶”ê°€í•´ì•¼ í•¨",
        source=solution_id
    )
    print(f"   Learning ID: {learning_id}")

    # í…ŒìŠ¤íŠ¸ 9: í†µê³„
    print("\n9. í†µê³„")
    stats = ctx.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # í…ŒìŠ¤íŠ¸ 10: ê´€ë ¨ ë…¸ë“œ ì¡°íšŒ
    print("\n10. ê´€ë ¨ ë…¸ë“œ ì¡°íšŒ (error)")
    related = ctx.get_related_nodes(error_id)
    print(f"    Caused by: {len(related['caused_by'])}ê°œ")
    print(f"    Resolved by: {len(related['resolved_by'])}ê°œ")

    # í…ŒìŠ¤íŠ¸ 11: Compact Summary
    print("\n11. Compact Summary")
    summary = ctx.generate_compact_summary()
    print(summary)

    # í…ŒìŠ¤íŠ¸ 12: íŒŒì¼ í¬ê¸° í†µê³„
    print("\n12. íŒŒì¼ í¬ê¸° ë° ì œí•œ í†µê³„")
    file_stats = ctx.get_file_stats()
    for key, value in file_stats.items():
        print(f"    {key}: {value}")

    # í…ŒìŠ¤íŠ¸ 13: ê°•ì œ ì •ë¦¬ (LOW importanceë§Œ)
    print("\n13. ê°•ì œ ì •ë¦¬ í…ŒìŠ¤íŠ¸")
    # LOW importance ë…¸ë“œ ì¶”ê°€
    for i in range(5):
        ctx.record_analysis(
            f"í…ŒìŠ¤íŠ¸ ë¶„ì„ {i}",
            importance=Importance.LOW
        )
    print(f"    ì •ë¦¬ ì „ ë…¸ë“œ ìˆ˜: {len(ctx.nodes)}")
    cleanup_result = ctx.force_cleanup(keep_high_only=False)
    print(f"    ì •ë¦¬ í›„ ë…¸ë“œ ìˆ˜: {cleanup_result['after_nodes']}")
    print(f"    ì‚­ì œëœ ë…¸ë“œ: {cleanup_result['removed_nodes']}")

    # í…ŒìŠ¤íŠ¸ 14: ì„¸ì…˜ ë³µì› ì •ë³´
    print("\n14. ì„¸ì…˜ ë³µì› ì •ë³´")
    session_info = ctx.get_session_info()
    print(f"    has_context: {session_info.get('has_context')}")
    print(f"    node_count: {session_info.get('node_count')}")
    print(f"    high_importance: {session_info.get('high_importance_count')}")
    print(f"    should_restore: {ctx.should_restore()}")

    # í…ŒìŠ¤íŠ¸ 15: ë³µì› í”„ë¡¬í”„íŠ¸ ìƒì„±
    print("\n15. ë³µì› í”„ë¡¬í”„íŠ¸ ìƒì„±")
    restoration_prompt = ctx.get_restoration_prompt()
    print(f"    í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(restoration_prompt)} ë¬¸ì")
    if restoration_prompt:
        # ì²˜ìŒ 500ìë§Œ ì¶œë ¥
        preview = restoration_prompt[:500] + "..." if len(restoration_prompt) > 500 else restoration_prompt
        print(f"    ë¯¸ë¦¬ë³´ê¸°:\n{preview}")

    # í…ŒìŠ¤íŠ¸ 16: check_restorable_session í•¨ìˆ˜
    print("\n16. check_restorable_session í•¨ìˆ˜")
    result = check_restorable_session()
    print(f"    restorable: {result.get('restorable')}")
    if result.get("session_info"):
        print(f"    requests: {result['session_info'].get('requests', [])}")

    # === v13.0 í…ŒìŠ¤íŠ¸ ===

    # í…ŒìŠ¤íŠ¸ 17: CircuitBreaker
    print("\n17. CircuitBreaker í…ŒìŠ¤íŠ¸")
    cb = CircuitBreaker(max_failures=3)
    action1 = cb.record_failure("task_1", "ì²« ë²ˆì§¸ ì—ëŸ¬")
    print(f"    1ì°¨ ì‹¤íŒ¨: {action1}")  # RETRY
    action2 = cb.record_failure("task_1", "ë‘ ë²ˆì§¸ ì—ëŸ¬")
    print(f"    2ì°¨ ì‹¤íŒ¨: {action2}")  # ALTERNATE_APPROACH
    action3 = cb.record_failure("task_1", "ì„¸ ë²ˆì§¸ ì—ëŸ¬")
    print(f"    3ì°¨ ì‹¤íŒ¨: {action3}")  # ESCALATE_TO_ARCHITECT

    summary = cb.get_failure_summary("task_1")
    print(f"    ì‹¤íŒ¨ ìš”ì•½: count={summary['failure_count']}, rec={summary['recommendation']}")

    cb.reset("task_1")
    print(f"    ë¦¬ì…‹ í›„: {cb.get_failure_summary('task_1')['failure_count']}")

    # í…ŒìŠ¤íŠ¸ 18: ContextManager CircuitBreaker í†µí•©
    print("\n18. ContextManager CircuitBreaker í†µí•©")
    ctx.record_error("í…ŒìŠ¤íŠ¸ ì—ëŸ¬ 1", context="í…ŒìŠ¤íŠ¸ ìƒí™©")
    action = ctx.record_failure("test_task", error_id)
    print(f"    record_failure ê²°ê³¼: {action}")
    ctx.record_success("test_task")
    print("    record_success ì™„ë£Œ")

    # í…ŒìŠ¤íŠ¸ 19: Verification Protocol
    print("\n19. Verification Protocol í…ŒìŠ¤íŠ¸")
    verify_id = ctx.record_verification("BUILD", True, "npm run build: SUCCESS")
    print(f"    Verification ID: {verify_id}")
    ctx.record_verification("TEST", False, "pytest: 1 test failed")
    ctx.record_verification("LINT", True, "ruff check: no issues")

    status = ctx.get_verification_status()
    for check, info in status.items():
        print(f"    {check}: passed={info['passed']}, fresh={info['fresh']}, age={info['age_seconds']}s")

    fresh = ctx.is_verification_fresh("BUILD")
    print(f"    BUILD fresh: {fresh}")

    # í…ŒìŠ¤íŠ¸ 20: Notepad Wisdom ë‚´ë³´ë‚´ê¸°
    print("\n20. Notepad Wisdom ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸")
    export_counts = ctx.export_to_wisdom("test_plan")
    print(f"    ë‚´ë³´ë‚´ê¸° ê²°ê³¼: {export_counts}")

    # í…ŒìŠ¤íŠ¸ 21: Escalation Summary
    print("\n21. Escalation Summary í…ŒìŠ¤íŠ¸")
    # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ìƒí™© ë§Œë“¤ê¸°
    for i in range(3):
        ctx.circuit_breaker.record_failure("escalate_task", f"ì—ëŸ¬ {i+1}")
    escalation = ctx.get_escalation_summary()
    print(f"    Escalation Summary ê¸¸ì´: {len(escalation)} ë¬¸ì")
    if "escalate_task" in escalation:
        print("    escalate_taskê°€ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ëª©ë¡ì— í¬í•¨ë¨")

    print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
